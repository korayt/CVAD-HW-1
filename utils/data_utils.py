from itertools import permutations

import numpy as np
import torch

from .mappings import LABEL_MAPPING

map_semantic_labels = np.vectorize(lambda x: LABEL_MAPPING[x])


def preprocess_semantic(semantic_img):
    semantic_img = np.frombuffer(semantic_img.raw_data, dtype=np.dtype("uint8"))
    dim = int(np.sqrt(semantic_img.shape[0] / 4))
    semantic_img = np.reshape(semantic_img, (dim, dim, 4))
    semantic_img = semantic_img[:, :, 2]
    semantic_img = map_semantic_labels(semantic_img)

    semantic_map = np.zeros((4, *semantic_img.shape))
    for i in range(4):
        semantic_map[i, :, :] = semantic_img == (i + 1)

    return semantic_map


def parse_carla_image(img):
    """Convert carla.Image to a RGB ordered numpy array."""
    img = np.frombuffer(img.raw_data, dtype=np.dtype("uint8"))
    # Calculate img dims assuming it's square
    dim = int(np.sqrt(img.shape[0] / 4))
    # Discard alpha channel and reverse BGR to RGB
    img = np.reshape(img, (dim, dim, 4))[:, :, 2::-1]
    img = np.copy(img)
    return img


def intersect(box_a, box_b):
    """ We resize both tensors to [A,B,2] without new malloc:
    [A,2] -> [A,1,2] -> [A,B,2]
    [B,2] -> [1,B,2] -> [A,B,2]
    Then we compute the area of intersect between box_a and box_b.
    Args:
      box_a: (tensor) bounding boxes, Shape: [A,4].
      box_b: (tensor) bounding boxes, Shape: [B,4].
    Return:
      (tensor) intersection area, Shape: [A,B].
    """
    A = box_a.size(0)
    B = box_b.size(0)
    max_xy = torch.min(box_a[:, 2:].unsqueeze(1).expand(A, B, 2),
                       box_b[:, 2:].unsqueeze(0).expand(A, B, 2))
    min_xy = torch.max(box_a[:, :2].unsqueeze(1).expand(A, B, 2),
                       box_b[:, :2].unsqueeze(0).expand(A, B, 2))
    inter = torch.clamp((max_xy - min_xy), min=0)
    return inter[:, :, 0] * inter[:, :, 1]


def get_box_area(bboxes):
    """Compute the area of a bbox.
    Args:
        box: (tensor) Shape: [num_boxes,4]
    Return:
        area: (tensor) Shape: [num_boxes]
    """
    area = ((bboxes[:, 2] - bboxes[:, 0])
            * (bboxes[:, 3] - bboxes[:, 1])).unsqueeze(1)
    return area


def preprocess_bboxes(bboxes, img_width, img_height, augment=True):
    """Convert yolo style bboxes to kitti style and filter unwanted ones."""
    def filter_small(bboxes, areas, threshold=0.004):
        """Filter bounding boxes that are smaller than a threshold."""
        cond = areas[:, 0] > threshold
        return bboxes[cond, :], areas[cond, :]

    def filter_offscreen(bboxes, areas):
        """Filter out bboxes that extend significantly outside the img boundaries."""
        cond = torch.logical_and((bboxes[:, 1:3] > -0.1), (bboxes[:, 3:5] < 1.1)).all(dim=1)
        return bboxes[cond, :], areas[cond, :]

    def filter_occluded(bboxes, areas, threshold=0.6):
        """Filter bboxes that are occluded significantly."""
        inters = intersect(bboxes[:, 1:], bboxes[:, 1:])
        cond = torch.ones((bboxes.shape[0]), dtype=torch.bool)
        for i, j in permutations(range(bboxes.shape[0]), 2):
            cur_inter = inters[i, j]
            if cur_inter / areas[i, 0] > threshold and areas[i, 0] < areas[j, 0] and cond[j]:
                cond[i] = False

        return bboxes[cond, :], areas[cond, :]

    def jiggle_bbox(bboxes, stddev=0.002):
        """Move and resize bboxes slightly for data augmentation."""
        bboxes[:, 1:] += torch.randn_like(bboxes[:, 1:]) * stddev
        return bboxes

    def filter_degenerate(bboxes, areas):
        """Filter out bboxes that have x0 > x1 or y0 > y1."""
        cond = torch.logical_and(bboxes[:, 1] < bboxes[:, 3],
                                 bboxes[:, 2] < bboxes[:, 4])
        return bboxes[cond, :], areas[cond, :]

    bboxes = torch.tensor(bboxes)
    # Compute areas from yolo width height values
    areas = (bboxes[:, 3] * bboxes[:, 4]).unsqueeze(-1)

    # Convert bboxes to kitti format but scaled to the range [0, 1]
    half_width = bboxes[:, 3] / 2
    half_height = bboxes[:, 4] / 2
    bboxes[:, 1] = bboxes[:, 1] - half_width
    bboxes[:, 2] = bboxes[:, 2] - half_height
    bboxes[:, 3] = bboxes[:, 1] + 2 * half_width
    bboxes[:, 4] = bboxes[:, 2] + 2 * half_height

    # Do filtering on invalid/difficult bboxes
    bboxes, areas = filter_small(bboxes, areas)
    bboxes, areas = filter_offscreen(bboxes, areas)
    bboxes, areas = filter_occluded(bboxes, areas)

    # Augmentation
    if augment:
        bboxes = jiggle_bbox(bboxes)

    # Filter invalid bounding boxes generated by data augmentation
    bboxes, areas = filter_degenerate(bboxes, areas)

    # 0 represents background, add 1 to all labels
    labels = bboxes[:, 0].long() + 1
    # Scale up coordinates to the range [0, img_width/img_height]
    bboxes[:, 1] *= img_width
    bboxes[:, 2] *= img_height
    bboxes[:, 3] *= img_width
    bboxes[:, 4] *= img_height

    return labels, bboxes[:, 1:]
